{
  "name": "Awesome-korean-nlp",
  "tagline": ":book: A curated list of resources dedicated to Natural Language Processing for Korean",
  "body": "# awesome-korean-nlp [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\r\n\r\n> A curated list of resources dedicated to Natural Language Processing for Korean\r\n>\r\n> 한글, 한국어에 대한 자연어처리를 하는데에 유용한 자료 목록을 모아두었습니다. \r\n>\r\n> Maintainers - [Insik Kim](https://github.com/insikk)\r\n>\r\n> Thanks to [Keon Kim](https://github.com/keonkim), [Martin Park](https://github.com/outpark) for making awesome nlp, which is original awesome-nlp\r\n\r\nPlease feel free to [pull requests](https://github.com/insikk/awesome-nlp/pulls), or email Insik Kim (insik92@gmail.com) to add links.\r\n\r\n\r\n## Table of Contents\r\n\r\n - [Tutorials and Courses](#tutorials-and-courses)\r\n   - [videos](#videos)\r\n - [Dataset](#dataset)\r\n - [Deep Learning for NLP](#deep-learning-for-nlp)\r\n - [Packages](#packages)\r\n   - [Implemendations](#implementations)\r\n   - [Libraries](#libraries)\r\n     - [Python](#user-content-python)\r\n     - [C++](#user-content-c++)\r\n   - [Services](#services)\r\n - [Articles](#articles)\r\n   - [Review Articles](#review-articles)\r\n   - [Word Vectors](#word-vectors)\r\n   - [Thought Vectors](#thought-vectors)\r\n   - [Machine Translation](#machine-translation)\r\n   - [General Natural Language Processing](#general-natural-langauge-processing)\r\n   - [Named Entity Recognition](#name-entity-recognition)\r\n   - [Single Exchange Dialogs](#single-exchange-dialogs)\r\n   - [Memory and Attention Models](#memory-and-attention-models)\r\n   - [General Natural Language Processing](#general-natural-language-processing)\r\n   - [Named Entity Recognition](#named-entity-recognition)\r\n   - [Neural Network](#neural-network)\r\n   - [Supplementary Materials](#supplementary-materials)\r\n - [Projects](#projects)\r\n - [Blogs](#blogs)\r\n - [Credits](#credits)\r\n\r\n\r\n## Tutorials and Courses\r\n\r\n* Tensor Flow Tutorial on [Seq2Seq](https://www.tensorflow.org/tutorials/seq2seq/index.html) Models\r\n* Natural Language Understanding with Distributed Representation [Lecture Note](https://github.com/nyu-dl/NLP_DL_Lecture_Note) by Cho\r\n\r\n### videos\r\n\r\n## Dataset\r\n\r\n### Corpus, 말뭉치\r\n\r\n* [세종말뭉치](https://ithub.korean.go.kr/user/guide/corpus/guide1.do)\r\n* [Wikipedia Korean](https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4_%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C)\r\n* [나무 위키](https://namu.mirror.wiki/w/%EB%82%98%EB%AC%B4%EC%9C%84%ED%82%A4:%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EB%8D%A4%ED%94%84)\r\n\r\n\r\n## Deep Learning for NLP \r\n\r\n\r\n## Packages\r\n\r\n### Implementations\r\n* [Pre-trained word embeddings for WSJ corpus](https://github.com/ai-ku/wvec) by Koc AI-Lab\r\n* [Word2vec](https://code.google.com/archive/p/word2vec) by Mikolov\r\n* [HLBL language model](http://metaoptimize.com/projects/wordreprs/) by Turian\r\n* [Real-valued vector \"embeddings\"](http://www.cis.upenn.edu/~ungar/eigenwords/) by Dhillon\r\n* [Improving Word Representations Via Global Context And Multiple Word Prototypes](http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes) by Huang\r\n* [Dependency based word embeddings](https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/)\r\n* [Global Vectors for Word Representations](http://nlp.stanford.edu/projects/glove/)\r\n\r\n### Libraries\r\n\r\n* <a id=\"python\">**Python** - Python NLP Libraries</a>\r\n  * [KoNLPy](http://konlpy.org) - A Python package for Korean natural language processing.\r\n\r\n* <a id=\"c++\">**C++** - C++ Libraries</a>\r\n  * [Mecab (Korean)](http://eunjeon.blogspot.com/)\r\n  \r\n### Services\r\n\r\n## Articles\r\n\r\n### Review Articles\r\n\r\n### Word Vectors\r\nResources about word vectors, aka word embeddings, and distributed representations for words.  \r\nWord vectors are numeric representations of words that are often used as input to deep learning systems. This process is sometimes called pretraining.  \r\n\r\n* [word2vec 관련 이론 정리](https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/)\r\n* [word2vec 튜토리얼](https://github.com/krikit/word2vec_tutorial/blob/master/word2vec_tutorial.ipynb)\r\n\r\n### Thought Vectors\r\nThought vectors are numeric representations for sentences, paragraphs, and documents.  The following papers are listed in order of date published, each one replaces the last as the state of the art in sentiment analysis.  \r\n\r\n\r\n### Single Exchange Dialogs\r\n\r\n### Memory and Attention Models\r\n\r\n### General Natural Language Processing\r\n\r\n### Named Entity Recognition\r\n\r\n### Neural Network\r\n\r\n### Supplementary Materials\r\n\r\n## Projects\r\n\r\n* [시인 뉴럴](https://github.com/carpedm20/poet-neural). Multi-layer LSTM for character-level language models in Torch. implemented by Kim Tae Hoon.\r\n* [한글 word2vec Demo](http://w.elnn.kr/search/). implemented by Daegeun Lee.\r\n\r\n## Blogs\r\n\r\n## Credits\r\npart of the lists are from \r\n* [ai-reading-list](https://github.com/m0nologuer/AI-reading-list) \r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}